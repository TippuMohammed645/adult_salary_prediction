{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import uniform,stats,chi2_contingency\n",
    "import joblib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold,RandomizedSearchCV,GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=pd.read_csv(r\"C:\\Users\\Tippu\\Downloads\\adult_census\\adult.csv\")\n",
    "data.sample(10)\n",
    "data.info()\n",
    "data2=data.copy()\n",
    "\n",
    "data.drop(['education'],axis=1,inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['salary']=data['salary'].apply(lambda x:1 if x== ' >50K' else 0)\n",
    "data['salary'].value_counts()\n",
    "\n",
    "#defining the replacements in dictionary\n",
    "#replacements={' >50K':1,' <=50K':0}\n",
    "#data['salary']=data['salary'].replace(replacements)\n",
    "# the output variable is unbalanced as mostly salary is below 50k more than 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(data,y='salary',x='hours-per-week')\n",
    "fig.show()\n",
    "\n",
    "#data[data['country']==' ?']\n",
    "a=data['age'].value_counts()\n",
    "fig=px.bar(a)\n",
    "fig.show()\n",
    "# slightly right skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in num_data.columns:\n",
    "   # fig=px.histogram(num_data[col])\n",
    "   # fig.show()\n",
    "    \n",
    "    \n",
    "#histogram of 'country','workclass','occupation'.\n",
    "c=['country','workclass','occupation']\n",
    "for col in c:\n",
    "    fig=px.histogram(data[col])\n",
    "    fig.show()\n",
    "    \n",
    "data['country'].value_counts()\n",
    "data['country'].value_counts(normalize=True)\n",
    "\n",
    "data['country'].unique()\n",
    "\n",
    "#imputing the \"?\" with \"other_nation\" as it only accounts to 1.7% of data\n",
    "#data['country'].replace(' ?','other_nation',inplace=True)\n",
    "#data['country'].unique()\n",
    "data['country'].value_counts(normalize=True) #1.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.histogram(data,'sex')\n",
    "fig.show()\n",
    "fig=px.histogram(data,'race')\n",
    "fig.show()\n",
    "\n",
    "fig=px.histogram(data,'relationship')\n",
    "fig.show()\n",
    "fig=px.histogram(data,'marital-status')\n",
    "fig.show()\n",
    "px.histogram(data['fnlwgt'],nbins=50).show()\n",
    "data['fnlwgt'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "relationship and marital statu( individual's life circumstances, )s both refer to aspects of an \n",
    "individual's personal life.employers may consider marital status when offering benefits such as health\n",
    "insurance, whereas relationship status may not have the same legal implications but could still \n",
    "influence an individual's lifestyle and priorities.\n",
    "individuals who are married or in long-term relationships may be perceived as more stable\n",
    "and reliable, both partners contribute to the household income potentially higher living standards. \n",
    "work-life balance ,financial responsibilities may influence their career choices ,\n",
    "Gender dynamics within relationships can also play a role in salary differences. \n",
    "\n",
    "\n",
    "Feature importance techniques, such as  permutation importance or model-specific feature importance, \n",
    "can help identify the influence of these variables on salary predictions.\n",
    "\n",
    " the effect of marital status on salary may differ based on gender, education level, or industry.\n",
    " Interaction terms can be included in the model to capture these complex relationships.\n",
    " : After training the predictive model, it's essential to interpret the results to understand\n",
    " how relationship status and marital status contribute to salary predictions. \n",
    "  Techniques such as partial dependence plots, SHAP (SHapley Additive exPlanations) values,or coefficient\n",
    "  analysis in linear models can help interpret the impact of these variables on salary predictions\n",
    "  \n",
    "  \n",
    "Accounting for Bias: Analyzing the influence of relationship status and marital status on salary \n",
    "predictions should consider potential biases. It's crucial to ensure that the predictive model does not \n",
    "perpetuate or amplify existing biases related to these variables. Techniques such as fairness-aware\n",
    "machine learning or bias mitigation strategies can be applied to address bias in the predictive model.\n",
    "\n",
    "maintaining the categorical nature of the variable\n",
    "  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "The output of the fit_transform() method of the OneHotEncoder is a sparse matrix by default, \n",
    "unless you specify sparse=False. Sparse matrices are often used for efficiency when dealing \n",
    "with large datasets with many categories, as they only store non-zero values.\n",
    "#when you create a DataFrame directly from the output of fit_transform(), it retains the\n",
    "sparse matrix format, resulting in a DataFrame with a single column containing the sparse \n",
    "matrix objects.\n",
    "\n",
    "#To convert the sparse matrix to a dense array and create a DataFrame with multiple columns\n",
    "representing the one-hot encoded features, you can use the toarray() method of the sparse \n",
    "matrix. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting input and output variables\n",
    "x=data.drop('salary',axis=1)\n",
    "y=data['salary']\n",
    "x.info()\n",
    "\n",
    "#spliting the numeric and categoric data\n",
    "num_data=x.select_dtypes(exclude='object')\n",
    "print(num_data.columns)\n",
    "\n",
    "cat_data=x.select_dtypes(exclude='int64')\n",
    "print(cat_data.columns)\n",
    "\n",
    "num_data.sample(5)\n",
    "cat_data[cat_data==' ?'].isnull().sum()\n",
    "cat_data.sample(5)\n",
    "\n",
    "data['country'].unique() # ,workclass,country,occupation use one leave out technique/target encoding  to encode the categ data\n",
    "data['relationship'].unique()  # sex,race,marital-status(7),relationship(6)  1hot encoding\n",
    "\n",
    "for col in cat_data.columns:\n",
    "    c=cat_data[col].unique()\n",
    "    print(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['occupation'].nunique() #15\n",
    "data['relationship'].nunique() #6\n",
    "data['marital-status'].nunique()#7\n",
    "data['race'].nunique() #5\n",
    "for col in cat_data.columns:\n",
    "   a= data[col].nunique()\n",
    "   print(f'{col} unique values :  {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data[num_data==' ?'].isnull().sum() # no question mark is observed or null values\n",
    "# QQ plot is a graphical tool to compare two probability distributions by plotting their quantiles\n",
    "# against each other.It's commonly used to assess if two datasets come from populations with a similar distribution. \n",
    "\n",
    "for col in num_data.columns:\n",
    "    stats.probplot(num_data[col],dist='norm',plot=plt)\n",
    "    plt.title('QQ Plot - Normal Distribution')\n",
    "    plt.xlabel('Theoretical Quantiles')\n",
    "    plt.ylabel('Sample Quantiles')\n",
    "    plt.grid(True)\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "    # mostly the numeric data closer to  normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOX PLOT \n",
    "for col in num_data.columns:\n",
    "    fig=px.box(num_data[col])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the OneHotEncoder instance\n",
    "encoder_instance = OneHotEncoder(drop='first')\n",
    "\n",
    "# here we also have to fit the instance so there wont face issue inetting feature names properly\n",
    "\n",
    "# variables to be encoded \n",
    "encoding_variables = ['sex', 'race', 'marital-status', 'relationship']\n",
    "\n",
    "# pipeline \n",
    "pipe = Pipeline(steps=[('onehot', encoder_instance.fit(x[encoding_variables]))])\n",
    "\n",
    "# applying column transformer \n",
    "preprocessor = ColumnTransformer(transformers=[('cat_encoding1', pipe, encoding_variables)])\n",
    "\n",
    "# fit the pipeline to the training data \n",
    "fit_preprocessor = preprocessor.fit(x[encoding_variables])\n",
    "\n",
    "# dump the preprocessing model\n",
    "#joblib.dump(fit_preprocessor, \"1hot_processor\")\n",
    "\n",
    "# Transform the data\n",
    "transformed_data = fit_preprocessor.transform(x[encoding_variables]).toarray()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = encoder_instance.get_feature_names_out(encoding_variables)\n",
    "\n",
    "# Create a DataFrame with the transformed data and feature names\n",
    "encoded_data = pd.DataFrame(transformed_data, columns=feature_names)\n",
    "encoded_data \n",
    "\n",
    "# concating the encoded data\n",
    "enc_data=pd.concat([cat_data,encoded_data],axis=1)\n",
    "enc_data.drop(['sex', 'race', 'marital-status', 'relationship'],axis=1,inplace=True)\n",
    "enc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the numeric data \n",
    "scale_instance=MinMaxScaler()\n",
    "scale_pipeline=Pipeline(steps=[('scale_num',scale_instance)])\n",
    "scaled_fit_model=scale_pipeline.fit(num_data)\n",
    "#save the pipeline\n",
    "#joblib.dump(scaled_fit_model,'scale_num')\n",
    "\n",
    "# transform the numeric data \n",
    "scaled_data=pd.DataFrame(scaled_fit_model.transform(num_data),columns=num_data.columns)\n",
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinate the encoded and scaled data leaving workclass, occupation and country \n",
    "a1=pd.concat([scaled_data,enc_data,y],axis=1)\n",
    "px.histogram(a1['country']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chisquared test -- to check the association between categorical features.if p<0.05 it indicates the features are associated\n",
    "#tetrachoric correlation-- for binary categorical variables \n",
    "#polychoric corr--for ordinal categorical variables\n",
    "#\n",
    "contingency_table=pd.crosstab(a1['occupation'],a1['workclass'])\n",
    "\n",
    "chi2,p,dof,expected=chi2_contingency(contingency_table)\n",
    "print(\"chi_squared statistic,pvalue\",chi2,p)\n",
    "\n",
    "\n",
    "#as pvalue=0 , it suggests strong evidence against the null hypothesis of independence \n",
    "contingency_table=pd.crosstab(a1['occupation'],a1['country'])\n",
    "chi2,p,dof,expected=chi2_contingency(contingency_table)\n",
    "print(\"chi_squared statistic,pvalue\",chi2,p)\n",
    "\n",
    "contingency_table=pd.crosstab(a1['country'],a1['workclass'])\n",
    "chi2,p,dof,expected=chi2_contingency(contingency_table)\n",
    "print(\"chi_squared statistic,pvalue\",chi2,p)\n",
    "\n",
    "# thus implies country,occupation,workclass have a strong association with one other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the indexes of the '?' rows in the dataset\n",
    "d_indexes1=a1[a1['workclass']==' ?'].index \n",
    "d_indexes2=a1[a1['occupation']==' ?'].index\n",
    "d_indexes3=a1[a1['country']==' ?'].index\n",
    "d=d_indexes1.append([d_indexes2,d_indexes3])\n",
    "\n",
    "#data with '?' variable data  \n",
    "data_qc=a1.loc[d]\n",
    "data_qc.drop_duplicates(inplace=True)\n",
    "data_qc.duplicated().sum()\n",
    "\n",
    "#without '?\" data\n",
    "data_wc=a1.drop(d)\n",
    "\n",
    "# target encode the workclass  without question data \n",
    "num_data.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3=data_wc[['workclass','occupation','country']]\n",
    "a2=data_wc.drop(a3,axis=1)\n",
    "a2  #encoded data without the country,occupation ,workclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(a2, a3['country'], test_size=0.2,random_state=42,stratify=a3['occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#approach 1\n",
    "\n",
    "instance4=DecisionTreeClassifier()\n",
    "#criterion='gini',splitter='best',random_state=42\n",
    "pre_model4=instance4.fit(X_train2, y_train2)\n",
    "prediction4=pre_model4.predict(X_test2)\n",
    "\n",
    "prediction4\n",
    "accuracy_score(y_test2,prediction4) #86%\n",
    " \n",
    "report4=classification_report(y_test2,prediction4)\n",
    "#print(\"Classification Report:\")\n",
    "print(report4)\n",
    "\n",
    "\n",
    "\n",
    "param_dist={'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'splitter':['best', 'random'],\n",
    "            'min_samples_split':[2,5,7,11,15,25,50],\n",
    "            'min_samples_leaf':[1,3,5,7,10,15],\n",
    "            'max_features':['int', 'float', 'sqrt', 'log2'],\n",
    "            'random_state':[0,21,42,61,100,250]\n",
    "            }\n",
    "\n",
    "instance_opt4=GridSearchCV(DecisionTreeClassifier(),param_dist,cv=5)\n",
    "\n",
    "#fit the model with randomization\n",
    "instance_opt4.fit(X_train2,y_train2)\n",
    "\n",
    "\n",
    "print(\"best_scores found :\",instance_opt4.best_score_)\n",
    "print(\"best_parameters found :\",instance_opt4.best_params_)\n",
    "#eatures': 'sqrt', 'min_samples_leaf': 15, 'min_samples_split': 2, 'random_state': 61, 'splitter': 'best'\n",
    "\n",
    "instance5=DecisionTreeClassifier(criterion= 'gini', max_features= 'sqrt', min_samples_leaf= 15, min_samples_split= 2,random_state=61,splitter= 'best')\n",
    "#criterion='gini',splitter='best',random_state=42\n",
    "pre_model5=instance5.fit(X_train2, y_train2)\n",
    "prediction5=pre_model4.predict(X_test2)\n",
    "\n",
    "prediction5\n",
    "accuracy_score(y_test2,prediction5) #86%\n",
    " \n",
    "report5=classification_report(y_test2,prediction5)\n",
    "#print(\"Classification Report:\")\n",
    "print(report5)\n",
    "\n",
    "\n",
    "print(\"best_parameters found :\",instance_opt4.best_params_)\n",
    "\n",
    "\n",
    "instance4=DecisionTreeClassifier(criterion= 'gini', max_features= 'log2', min_samples_leaf= 10, min_samples_split= 5, splitter= 'best',random_state=0)\n",
    "#criterion='gini',splitter='best',random_state=42\n",
    "pre_model4=instance4.fit(X_train2, y_train2)\n",
    "prediction4=pre_model4.predict(X_test2)\n",
    "\n",
    "prediction4\n",
    "accuracy_score(y_test2,prediction4) #86%\n",
    " \n",
    "report4=classification_report(y_test2,prediction4)\n",
    "#print(\"Classification Report:\")\n",
    "print(report4)\n",
    "\n",
    "# both give similar results though very much lack in macro avg scores  with 92% accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create KNN classifier\n",
    "k =5 # Number of neighbors\n",
    "knn_classifier_final = KNeighborsClassifier(weights='distance',n_neighbors=k,n_jobs=-1,p=2)\n",
    "# Train the classifier\n",
    "knn_classifier_final.fit(X_train2, y_train2)\n",
    "# Make predictions on the test set\n",
    "prediction2 = knn_classifier_final.predict(X_test2)\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test2, prediction2)\n",
    "print(\"Accuracy:\", accuracy) #91.32%  ; #f1score: macro-avg:0.08 ; wgted avg:0.89\n",
    "report1=classification_report(y_test2,prediction2)\n",
    "print(report1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the ? data in country variable \n",
    "d1=data_qc[data_qc['country']==' ?'] #582 values missing \n",
    "d2=d1.drop(['workclass','occupation','country'],axis=1)\n",
    "\n",
    "#predicting the  missing values of country \n",
    "a=pd.DataFrame()\n",
    "a['country']=knn_classifier_final.predict(d2)\n",
    "a.set_index(d1.index)\n",
    "\n",
    "#replace '?' with calculated values \n",
    "data_qc.loc[data_qc['country']==' ?','country']=a.values\n",
    "# It uses boolean indexing to identify and replace the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a1['country'].unique()\n",
    "note :\n",
    "    \n",
    "data_qc.loc[data_qc['country']==' ?']=a.values   #a.values is ndarray type \n",
    "\n",
    "#ValueError: Must have equal len keys and value when setting with an ndarray with misiing column name after boolean mask\n",
    "# suggests that the length of the keys (indices) and the length of the values\n",
    "# (array from a.values) do not match when you're trying to replace values in data_qc.\n",
    "\n",
    "\n",
    "#When you use .loc[] to select rows in a DataFrame, you need to ensure that \n",
    "# the replacement values have the same length and alignment as the selected \n",
    "# rows. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "checking the country column:\n",
    "d3=data_qc[data_qc['country']!=' ?']\n",
    "d3_y=pd.DataFrame()\n",
    "d3_y['country']=d3['country']\n",
    "s2=d3.drop(['workclass','occupation','country'],axis=1)\n",
    "s2_predicted=knn_classifier_final.predict(s2)\n",
    "accuracy = accuracy_score(d3_y['country'], s2_predicted)\n",
    "print(\"Accuracy:\", accuracy) #91.79%\n",
    "report1=classification_report(d3_y['country'], s2_predicted)\n",
    "print(report1) \n",
    "\n",
    "#performance is good for the major class and it is underperformed for less record class which is not good\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "\n",
    "instance3=LogisticRegression()\n",
    "\n",
    "pre_model3=instance3.fit(X_train2, y_train2)\n",
    "prediction3=pre_model3.predict(X_test2)\n",
    "accuracy_score(y_test2,prediction3) #92%\n",
    "\n",
    "report3=classification_report(y_test2,prediction3)\n",
    "#print(\"Classification Report:\")\n",
    "print(report3)\n",
    "\n",
    "param_dist={'penalty':['l1','l2','elasticnet'],\n",
    "            'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "             'max_iter':[50,100,250,500,1000],\n",
    "            }\n",
    "\n",
    "#n_iter--it determines how many different combinations of hyperparameters will be tried.\n",
    "#max_iter in param_dist represents the maximum number of iterations taken for the solvers to converge.\n",
    "instance_opt3=GridSearchCV(LogisticRegression(n_jobs=-1),param_dist,cv=5)\n",
    "#fit the model with randomization\n",
    "instance_opt3.fit(X_train2,y_train2)\n",
    "  #91.45%\n",
    "\n",
    "print(\"best_parameters found :\",instance_opt3.best_params_)\n",
    "print(\"best_scores found :\",instance_opt3.best_score_)\n",
    "#best_parameters found : {'C': 0.8751328233611145, 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'\n",
    "\n",
    "## Evaluate the model on the test set\n",
    "print(\"Test set score with best parameters: \",instance_opt3.score(X_test2, y_test2))\n",
    "\n",
    "instance3=LogisticRegression( max_iter= 50, penalty= 'l1', solver= 'saga',n_jobs=-1)\n",
    "#C= 0.8751328233611145, max_iter= 100, penalty= 'l2', solver= 'saga',n_jobs=-1\n",
    "#'max_iter': 50, 'penalty': 'l1', 'solver': 'saga'\n",
    "pre_model3=instance3.fit(X_train2, y_train2)\n",
    "prediction3=pre_model3.predict(X_test2)\n",
    "\n",
    "accuracy_score(y_test2,prediction3) #92%\n",
    "report3=classification_report(y_test2,prediction2)\n",
    "#print(\"Classification Report:\")\n",
    "print(report3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target encodng on country variable in data_Wc\n",
    "kf=KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "encoded_fold_values=[]\n",
    "#initialise the target encoder\n",
    "target_encoder=TargetEncoder(cols=['country'],smoothing=0.1)\n",
    "for train_index,test_index in kf.split(data_wc):\n",
    "    \n",
    "    train_data,val_data=data_wc.iloc[train_index],data_wc.iloc[test_index]\n",
    "    target_encoder.fit(train_data['country'],train_data['salary'])\n",
    "    \n",
    "    #apply target encoder on test data using trained encoder \n",
    "    val_encoded=target_encoder.transform(val_data['country'])\n",
    "    \n",
    "    # Handle unseen categories by filling with global mean\n",
    "    global_mean = train_data['salary'].mean()\n",
    "    val_encoded.fillna(global_mean, inplace=True)\n",
    "    \n",
    "    # store encoded values\n",
    "    encoded_fold_values.append(val_encoded)\n",
    "    # concatenate the encoded values from each fold into a single DataFrame \n",
    "all_encoded_values=pd.concat(encoded_fold_values)\n",
    "\n",
    "final_scores=all_encoded_values.groupby(by='country',level=-1).mean()\n",
    "final_scores\n",
    "#The encoding process is properly isolated within each fold of the cross-validation, preventing data leakage.\n",
    "# Unseen categories in the validation data are handled appropriately by filling them with the global mean of the target variable.\n",
    "# The mean encoded value for each category is calculated across all folds, providing a robust estimate of the category's encoded value.\n",
    "\n",
    "# encoded data without workclass and occupation of data_wc\n",
    "hnd_data=pd.concat([a2,final_scores],axis=1)\n",
    "hnd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To handle rare or unseen categories, smoothing techniques are often applied in target encoding\n",
    "#target encoding implementation and ensure that it is properly isolated within each fold of the cross-validation process. \n",
    "# Additionally, you may need to adjust parameters such as smoothing factor or regularization strength to control the number of unique encoded values generated.\n",
    "\n",
    "''' k-nearest neighbors (KNN) imputation or predictive modeling.encode them separately from other categories\n",
    "Apply target encoding with smoothing techniques to handle potential overfitting and reduce the impact of rare categories.\n",
    "Smoothing techniques like Laplace smoothing (additive smoothing) or James-Stein estimator\n",
    "can help mitigate the risk of overfitting by adding a small value to the frequency counts of each category.\n",
    "one-hot encoding, ordinal encoding, or binary encoding.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working on workclass data to handle the ' ?' missing values. \n",
    "# Split dataset\n",
    "X_traina, X_testa, y_traina, y_testa = train_test_split(hnd_data, a3['workclass'], test_size=0.2,random_state=42,stratify=a3['workclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                    # baseline model\n",
    "\n",
    "\n",
    "instance1=LogisticRegression()  #74% # macro avg(f1score)-0.13 !! wgt avg(f1score)-0.63\n",
    "pre_model=instance1.fit(X_traina, y_traina)\n",
    "prediction=pre_model.predict(X_testa)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_testa,prediction) \n",
    "report_a=classification_report(y_testa,prediction)\n",
    "#print(\"Classification Report:\")\n",
    "print(report_a)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "\n",
    "#########################finding the best parameters ########################################\n",
    "\n",
    "param_dist={'penalty':['l1','l2','elasticnet'],\n",
    "            'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "             'max_iter':[50,100,250,500,1000,2500]\n",
    "            }\n",
    "\n",
    "instance_opta=GridSearchCV(LogisticRegression(random_state=42,n_jobs=-1),param_dist,cv=5)\n",
    "\n",
    "#fit the model with randomization\n",
    "instance_opta.fit(X_traina,y_traina)\n",
    "     \n",
    "\n",
    "print(\"best_parameters found :\",instance_opta.best_params_)\n",
    "print(\"best_scores found :\",instance_opta.best_score_)\n",
    "#best_parameters found : {'max_iter': 50, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "# best_scores found : 0.738447536981248\n",
    "\n",
    "## Evaluate the model on the test set\n",
    "print(\"Test set score with best parameters: \",instance_opta.score(X_testa, y_testa))\n",
    "\n",
    "#########################based on parameters we got from gridsearch############################\n",
    "\n",
    "instance1=LogisticRegression(max_iter= 50, penalty= 'l2', solver= 'liblinear',n_jobs=-1,random_state=42)  #74%\n",
    "pre_model=instance1.fit(X_traina, y_traina)\n",
    "prediction=pre_model.predict(X_testa)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_testa,prediction)  #74%; 4 values present and other 3  zero scores;f1(macro avg)--0.13 !! f1score(wgt avg)--0.63\n",
    "report_a=classification_report(y_testa,prediction)\n",
    "#print(\"Classification Report:\")\n",
    "print(report_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_dist={'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'splitter':['best', 'random'],\n",
    "            'min_samples_split':[2,5,7,11,15,25,50],\n",
    "            'min_samples_leaf':[1,3,5,7,10,15],\n",
    "            'max_features':['int', 'float', 'sqrt', 'log2'],\n",
    "            'random_state':[0,21,42,61,100,250]\n",
    "            }\n",
    "\n",
    "instance_optb=GridSearchCV(DecisionTreeClassifier(),param_dist,cv=5)\n",
    "\n",
    "#fit the model with randomization\n",
    "instance_optb.fit(X_traina,y_traina)\n",
    "\n",
    "\n",
    "print(\"best_scores found :\",instance_optb.best_score_)\n",
    "print(\"best_parameters found :\",instance_optb.best_params_)\n",
    "#eatures': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 50, 'random_state': 100, 'splitter': 'random'  #74%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trail1\n",
    "print('trail1')\n",
    "instanceb=DecisionTreeClassifier(criterion= 'gini', max_features= 'sqrt', min_samples_leaf= 10, min_samples_split= 2,random_state=42,splitter= 'best')\n",
    "\n",
    "pre_modelb=instanceb.fit(X_traina, y_traina)\n",
    "predictionb=pre_modelb.predict(X_testa)\n",
    "\n",
    "predictionb\n",
    "accuracy_score(y_testa,predictionb) ##73%; 6 values present and 1 has zero scores;f1(macro avg)--0.17 !! f1score(wgt avg)--0.65\n",
    " \n",
    "reportb=classification_report(y_testa,predictionb)\n",
    "#print(\"Classification Report:\")\n",
    "print(reportb)\n",
    "\n",
    "#trail2\n",
    "#print('trail2')\n",
    "#instanceb=DecisionTreeClassifier(criterion= 'gini', max_features= 'sqrt', min_samples_leaf= 10, min_samples_split= 50,random_state=100,splitter= 'random')\n",
    "#pre_modelb=instanceb.fit(X_traina, y_traina)\n",
    "#predictionb=pre_modelb.predict(X_testa)\n",
    "\n",
    "#accuracy_score(y_testa,predictionb) #73%; 4 values present and other 3  zero scores;f1(macro avg)--0.12 !! f1score(wgt avg)--0.63\n",
    " \n",
    "#reportb=classification_report(y_testa,predictionb)\n",
    "#print(\"Classification Report:\")\n",
    "#print(reportb)\n",
    "\n",
    "# trail1 shows good results comparitively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset\n",
    "\n",
    "instancec=KNeighborsClassifier()  #71%\n",
    "pre_modelc=instancec.fit(X_traina, y_traina)\n",
    "predictionc=pre_modelc.predict(X_testa)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_testa,predictionc) \n",
    "reportc=classification_report(y_testa,predictionc)\n",
    "#print(\"Classification Report:\")\n",
    "print(reportc)\n",
    "#74%--f1score(macroavg)--0.18 ; wgtavg(f1score)--0.65\n",
    "\n",
    "\n",
    "#set zero_division parameter\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#model improve to increase its efficiency\n",
    "\n",
    "param_dist={'weights':['distance','uniform'],\n",
    "            'n_neighbors':[5,10,15,25],\n",
    "            'p':[1,2]\n",
    "            }\n",
    "instance_optc=GridSearchCV(KNeighborsClassifier(n_jobs=-1),param_dist,cv=5)\n",
    "#fit the model with randomization\n",
    "instance_optc.fit(X_traina,y_traina)\n",
    "print(\"best_parameters found :\",instance_optc.best_params_)\n",
    "print(\"best_scores found :\",instance_optc.best_score_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"Test set score with best parameters: \",instance_optc.score(X_testa, y_testa))\n",
    "\n",
    "\n",
    "# best parameters:{'n_neighbors': 25, 'p': 1, 'weights': 'uniform'} #74%\n",
    "\n",
    "#########################################################################################################################################################\n",
    "k =25 # Number of neighbors\n",
    "knn_classifier = KNeighborsClassifier(weights='uniform',n_neighbors=k,p=1,n_jobs=-1)\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_traina, y_traina)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictionc = knn_classifier.predict(X_testa)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_C = accuracy_score(y_testa, predictionc)\n",
    "print(\"Accuracy:\", accuracy_C)  #74%; 3 values present and other mostly zero scores;f1(macro avg)--0.14 !! f1score(wgt avg)--0.64\n",
    "\n",
    "reportc=classification_report(y_testa,predictionc)\n",
    "print(reportc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instanceb_svc=SVC(kernel='poly',gamma=0.1,C=0.05)\n",
    "pre_model_Svc=instanceb_svc.fit(X_traina, y_traina)\n",
    "prediction_svc=pre_model_Svc.predict(X_testa)\n",
    "\n",
    "accuracy_score(y_testa,prediction_svc) ##74%; 6 values  are not present and 1 has non-zero scores;f1(macro avg)--0.12 !! f1score(wgt avg)--0.63\n",
    " \n",
    "report_svc=classification_report(y_testa,prediction_svc)\n",
    "#print(\"Classification Report:\")\n",
    "print(report_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target enccode the country column in data_qc and predict workclass variable\n",
    "\n",
    "kf1=KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "encoded_fold_values1=[]\n",
    "#initialise the target encoder\n",
    "target_encoder1=TargetEncoder(cols=['country'],smoothing=0.1)\n",
    "\n",
    "\n",
    "for train_index,test_index in kf.split(data_qc):\n",
    "    \n",
    "    train_data1,val_data1=data_qc.iloc[train_index],data_qc.iloc[test_index]\n",
    "    \n",
    "    target_encoder1.fit(train_data1['country'],train_data1['salary'])\n",
    "    \n",
    "    #apply target encoder on test data using trained encoder \n",
    "    \n",
    "    val_encoded1=target_encoder1.transform(val_data1['country'])\n",
    "    \n",
    "    # Handle unseen categories by filling with global mean\n",
    "    global_mean1 = train_data1['salary'].mean()\n",
    "    val_encoded1.fillna(global_mean, inplace=True)\n",
    "    \n",
    "    # store encoded values\n",
    "    encoded_fold_values1.append(val_encoded1)\n",
    "    # concatenate the encoded values from each fold into a single DataFrame \n",
    "all_encoded_values1=pd.concat(encoded_fold_values1)\n",
    "final_scores1=all_encoded_values1.groupby(by='country',level=-1).mean()\n",
    "final_scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concating the encoded values with the data to make prediction\n",
    "d_qc=data_qc.drop(['country','occupation','workclass'],axis=1)\n",
    "data_qc1=pd.concat([d_qc,final_scores1],axis=1)\n",
    "\n",
    "\n",
    "w_indexes=data_qc[data_qc['workclass']==' ?'].index\n",
    "pred_data=data_qc1.loc[w_indexes]\n",
    "pred_data\n",
    "\n",
    "#predicting the worclass missing values\n",
    "predict_workclass=pd.DataFrame()\n",
    "predict_workclass['workclass']=pre_modelb.predict(pred_data)\n",
    "predict_workclass\n",
    "\n",
    "#replacing the  '?' with predicted values\n",
    "data_qc.loc[data_qc['workclass']==' ?','workclass']=predict_workclass.values\n",
    "\n",
    "data_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target encode the workclass variable and model the data \n",
    "\n",
    "kf1=KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "encoded_fold_values2=[]\n",
    "\n",
    "#initialise the target encoder\n",
    "target_encoder2=TargetEncoder(cols=['workclass'],smoothing=0.1)\n",
    "\n",
    "\n",
    "for train_index,test_index in kf.split(data_wc):\n",
    "    \n",
    "    train_data2,val_data2=data_wc.iloc[train_index],data_wc.iloc[test_index]\n",
    "    \n",
    "    target_encoder2.fit(train_data2['workclass'],train_data2['salary'])\n",
    "    \n",
    "    #apply target encoder on test data using trained encoder \n",
    "    \n",
    "    val_encoded2=target_encoder2.transform(val_data2['workclass'])\n",
    "    \n",
    "    # Handle unseen categories by filling with global mean\n",
    "    global_mean2= train_data2['salary'].mean()\n",
    "    val_encoded2.fillna(global_mean, inplace=True)\n",
    "    \n",
    "    # store encoded values\n",
    "    encoded_fold_values2.append(val_encoded2)\n",
    "    # concatenate the encoded values from each fold into a single DataFrame \n",
    "all_encoded_values2=pd.concat(encoded_fold_values2)\n",
    "final_scores2=all_encoded_values2.groupby(by='workclass',level=-1).mean()\n",
    "final_scores2\n",
    "#########################################################################################################################################\n",
    "#concatinated the workclass encoded variable values to hnd_data\n",
    "hnd_data2=pd.concat([hnd_data,final_scores2],axis=1)\n",
    "hnd_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train2a, X_test2a, y_train2a, y_test2a = train_test_split(hnd_data2, a3['occupation'], test_size=0.2,random_state=42,stratify=a3['occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # baseline model\n",
    "#creating instance and training the data \n",
    "instance_2a=LogisticRegression()  #32.26% # macro avg(f1score)-0.16 !! wgt avg(f1score)-0.26--- 3/14(zeros)\n",
    "pre_model2a=instance_2a.fit(X_train2a, y_train2a)\n",
    "prediction2a=pre_model2a.predict(X_test2a)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_test2a,prediction2a) \n",
    "report_2a=classification_report(y_test2a,prediction2a)\n",
    "print(report_2a)\n",
    "\n",
    "\n",
    "instance_2b=KNeighborsClassifier()  #27% # macro avg(f1score)-0.18 !! wgt avg(f1score)-0.25--- 1/14(zeros)\n",
    "pre_model2b=instance_2b.fit(X_train2a, y_train2a)\n",
    "prediction2b=pre_model2b.predict(X_test2a)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_test2a,prediction2b) \n",
    "report_2b=classification_report(y_test2a,prediction2b)\n",
    "print(report_2b)\n",
    "\n",
    "\n",
    "instance_2c=DecisionTreeClassifier()  #23% # macro avg(f1score)-0.18 !! wgt avg(f1score)-0.24--- 1/14(zeros)\n",
    "pre_model2c=instance_2c.fit(X_train2a, y_train2a)\n",
    "prediction2c=pre_model2c.predict(X_test2a)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_test2a,prediction2c) \n",
    "report_2c=classification_report(y_test2a,prediction2c)\n",
    "print(report_2c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist={'penalty':['l1','l2','elasticnet'],\n",
    "            'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "             'max_iter':[50,100,250,500,1000,2500]\n",
    "            }\n",
    "\n",
    "instance_opt2a=GridSearchCV(LogisticRegression(random_state=42,n_jobs=-1),param_dist,cv=5)\n",
    "\n",
    "#fit the model with randomization\n",
    "instance_opt2a.fit(X_train2a,y_train2a)\n",
    "     \n",
    "\n",
    "print(\"best_parameters found :\",instance_opt2a.best_params_)\n",
    "print(\"best_scores found :\",instance_opt2a.best_score_)\n",
    "#best_parameters found : {'max_iter': 50, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "# best_scores found : 0.738447536981248\n",
    "\n",
    "## Evaluate the model on the test set\n",
    "print(\"Test set score with best parameters: \",instance_opt2a.score(X_test2a, y_test2a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters: 'max_iter': 50, 'penalty': 'l2', 'solver': 'lbfgs' #32.32%\n",
    "instance_2a=LogisticRegression(max_iter= 50, penalty= 'l2', solver= 'lbfgs')  #32.26% # macro avg(f1score)-0.16 !! wgt avg(f1score)-0.26(improved)--- 4/14(zeros)\n",
    "pre_model2a=instance_2a.fit(X_train2a, y_train2a)\n",
    "prediction2a=pre_model2a.predict(X_test2a)   # precision, recall values for some levels have improved significantly than base model \n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_test2a,prediction2a) \n",
    "report_2a=classification_report(y_test2a,prediction2a)\n",
    "#print(\"Classification Report:\")\n",
    "print(report_2a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist={'criterion':['gini', 'entropy', 'log_loss'],\n",
    "            'splitter':['best', 'random'],\n",
    "            'min_samples_split':[2,5,7,11,15,25,50],\n",
    "            'min_samples_leaf':[1,3,5,7,10,15],\n",
    "            'max_features':['int', 'float', 'sqrt', 'log2'],\n",
    "            'random_state':[0,21,42,61,100,250]\n",
    "            }\n",
    "\n",
    "instance_opt2b=GridSearchCV(DecisionTreeClassifier(),param_dist,cv=5)\n",
    "\n",
    "#fit the model with randomization\n",
    "instance_opt2b.fit(X_train2a,y_train2a)\n",
    "\n",
    "print(\"best_scores found :\",instance_opt2b.best_score_)\n",
    "print(\"best_parameters found :\",instance_opt2b.best_params_)\n",
    "#best_scores found : 0.3144351172083855\n",
    "# best_parameters found : {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 25, 'random_state': 61, 'splitter': 'random'}\n",
    "###########################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_2c=DecisionTreeClassifier(criterion= 'gini', max_features= 'sqrt', min_samples_leaf= 10, min_samples_split= 25, random_state= 61, splitter= 'random')  \n",
    "pre_model2c=instance_2c.fit(X_train2a, y_train2a)\n",
    "prediction2c=pre_model2c.predict(X_test2a)    #31% # macro avg(f1score)-0.17 !! wgt avg(f1score)-0.27--- 1/14(zeros)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_test2a,prediction2c) \n",
    "report_2c=classification_report(y_test2a,prediction2c)\n",
    "#print(\"Classification Report:\")\n",
    "print(report_2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist={'weights':['distance','uniform'],\n",
    "            'n_neighbors':[5,10,15,25],\n",
    "            'p':[1,2]  \n",
    "            }\n",
    "instance_opt2c=GridSearchCV(KNeighborsClassifier(n_jobs=-1),param_dist,cv=5)\n",
    "#fit the model with randomization\n",
    "instance_opt2c.fit(X_train2a,y_train2a)\n",
    "print(\"best_parameters found :\",instance_opt2c.best_params_)\n",
    "print(\"best_scores found :\",instance_opt2c.best_score_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"Test set score with best parameters: \",instance_opt2c.score(X_test2a, y_test2a))\n",
    "#######################################################################################################################################################\n",
    "instance_2bb=KNeighborsClassifier(n_neighbors= 25, p= 1, weights= 'uniform')  #27% # macro avg(f1score)-0.18 !! wgt avg(f1score)-0.25--- 1/14(zeros)\n",
    "pre_model2bb=instance_2b.fit(X_train2a, y_train2a)\n",
    "prediction2bb=pre_model2b.predict(X_test2a)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_test2a,prediction2bb) \n",
    "report_2bb=classification_report(y_test2a,prediction2bb)\n",
    "#print(\"Classification Report:\")\n",
    "print(report_2bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instance_svc=SVC(kernel='poly',gamma=0.45,C=0.35)  #33% # macro avg(f1score)-0.16 !! wgt avg(f1score)-0.26--- 4/14(zeros) \n",
    "instance_svc.fit(X_train2a, y_train2a)\n",
    "prediction_svc=instance_svc.predict(X_test2a)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_test2a,prediction_svc) \n",
    "report_svc=classification_report(y_test2a,prediction_svc)\n",
    "#print(\"Classification Report:\")\n",
    "print(report_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding the occupation column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encode=LabelEncoder()\n",
    "\n",
    "a3['occ_encoded']=label_encode.fit_transform(a3['occupation'])  #it  was performed as the o/p for training is expected to be numeric in xgboost\n",
    "a3['occ_encoded']\n",
    "#a3['occupation'].nunique() #14\n",
    "\n",
    "#display the name and encoded value\n",
    "for name,label in zip(a3['occupation'],a3['occ_encoded']):\n",
    "    print(f\"{name} -> {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train2aa, X_test2aa, y_train2aa, y_test2aa = train_test_split(hnd_data2, a3['occ_encoded'], test_size=0.2,random_state=42,stratify=a3['occ_encoded'])\n",
    "#xgboostclassifier\n",
    "\n",
    "\n",
    "instance_xaa=XGBClassifier()\n",
    "pre_model_xaa=instance_xaa.fit(X_train2aa, y_train2aa)\n",
    "prediction_xaa=pre_model_xaa.predict(X_test2aa)\n",
    "\n",
    "#metrics\n",
    "accuracy_score(y_test2aa,prediction_xaa)  #34%; f1(macro avg)--0.25 !! f1score(wgt avg)--0.32 # 1/14\n",
    "report_xaa=classification_report(y_test2aa,prediction_xaa)\n",
    "#print(\"Classification Report:\")\n",
    "print(report_xaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target encode the workclass variable and model the data \n",
    "\n",
    "kf1=KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "encoded_fold_values3=[]\n",
    "\n",
    "#initialise the target encoder\n",
    "target_encoder3=TargetEncoder(cols=['workclass'],smoothing=0.1)\n",
    "\n",
    "for train_index,test_index in kf.split(data_qc):\n",
    "    \n",
    "    train_data3,val_data3=data_qc.iloc[train_index],data_qc.iloc[test_index]\n",
    "    \n",
    "    target_encoder3.fit(train_data3['workclass'],train_data3['salary'])\n",
    "    \n",
    "    #apply target encoder on test data using trained encoder \n",
    "    \n",
    "    val_encoded3=target_encoder3.transform(val_data3['workclass'])\n",
    "    \n",
    "    # Handle unseen categories by filling with global mean\n",
    "    global_mean3= train_data3['salary'].mean()\n",
    "    val_encoded3.fillna(global_mean3, inplace=True)\n",
    "    \n",
    "    # store encoded values\n",
    "    encoded_fold_values3.append(val_encoded3)\n",
    "    # concatenate the encoded values from each fold into a single DataFrame \n",
    "all_encoded_values3=pd.concat(encoded_fold_values3)\n",
    "final_scores3=all_encoded_values3.groupby(by='workclass',level=-1).mean()\n",
    "final_scores3\n",
    "\n",
    "#concating the encoded values with the data to make prediction\n",
    "data_qc2=pd.concat([data_qc1,final_scores3],axis=1)\n",
    "\n",
    "#indexes and the ' ?' data for predicting the occupation variable \n",
    "w_indexes=data_qc[data_qc['occupation']==' ?'].index\n",
    "pred_data3=data_qc2.loc[w_indexes]\n",
    "pred_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the worclass missing values\n",
    "predict_occ=pd.DataFrame()\n",
    "predict_occ['occupation']=pre_model_xaa.predict(pred_data3)\n",
    "\n",
    "#inversing the label encoder values into its origianl form\n",
    "predict_occ['occupation']=label_encode.inverse_transform(predict_occ)\n",
    "predict_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the  '?' with predicted values\n",
    "data_qc.loc[data_qc['occupation']==' ?','occupation']=predict_occ.values\n",
    "\n",
    "# concate the missing data with its other part\n",
    "data_final=pd.concat([data_wc,data_qc],axis=0)\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the missing data with its other part\n",
    "data_f=pd.concat([data_wc,data_qc],axis=0)\n",
    "data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f1=data_f[['workclass','occupation','country']]\n",
    "data1=data.copy()\n",
    "\n",
    "#removing missing values columns in the data1\n",
    "data1_=data1.drop(['workclass','occupation','country'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#concating the handled missing value data with original dataset\n",
    "data_final=pd.concat([data1_,data_f1],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final[data_final==' ?'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump the handled adult census data \n",
    "\n",
    "joblib.dump(data_final,'modified_adult_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
